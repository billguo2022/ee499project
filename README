
Acknowledgement
Since the purpose of the class is on machine learning, also due to time constraints, I used Mr. Zhengxia Zhouâ€™s environment for the project (the environment contains the rocket dynamics animation drawing). The learning and policy part of the project is completely my own work.This project presents Deep Q learning while the original project uses Actor Critic learning. Deep Q learning  is advantageous over Actor-Critic in terms of being more robust to noisy data, having more stable training, and being able to handle large state space. 

Policy code: 
MLP (Multilayer Perceptron) class:
This class defines a feedforward neural network architecture that takes an input of input_dim dimensions and outputs a vector of output_dim dimensions.
class MLP(nn.Module):


   def __init__(self, input_dim, output_dim):
       super().__init__()


       h_dim = 128


       self.linear1 = nn.Linear(in_features=input_dim, out_features=h_dim, bias=True)
       self.linear2 = nn.Linear(in_features=h_dim, out_features=h_dim, bias=True)
       self.linear3 = nn.Linear(in_features=h_dim, out_features=output_dim, bias=True)
       self.relu = nn.LeakyReLU(0.2)


   def forward(self, x):
       x = self.relu(self.linear1(x))
       x = self.relu(self.linear2(x))
       x = self.linear3(x)
       return x

In the constructor, three linear (fully-connected) layers and a leaky ReLU activation function are defined. The first two layers use h_dim hidden units, and the last layer has output_dim units. The leaky ReLU activation function will be applied after the first and second linear layers.
In the forward method, the input x is passed through the first linear layer, then the activation function, then the second linear layer, the activation function again, and finally the third linear layer. The output of the last linear layer is returned.
DQN (Deep Q-Network) class:
This class uses two instances of the MLP class to form the main Q-network and the target Q-network, following the DQN algorithm.
class DQN(nn.Module):
  
   def __init__(self, input_dim, output_dim):
       super().__init__()


       self.output_dim = output_dim
       self.q_network = MLP(input_dim=input_dim, output_dim=output_dim)
       self.target_q_network = MLP(input_dim=input_dim, output_dim=output_dim)
       self.update_target_network()
       self.target_q_network.eval()


       self.optimizer = optim.RMSprop(self.q_network.parameters(), lr=5e-5)
       self.gamma = 0.99  # add this line to set the discount factor




   def forward(self, x):
       return self.q_network(x)


   def get_action(self, state, epsilon=0.1):
       if random.random() < epsilon:  # exploration
           action_id = random.randint(0, self.output_dim - 1)
       else:  # exploitation
           state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
           q_values = self.forward(state)
           action_id = q_values.argmax(dim=1).item()


       return action_id


   def update_target_network(self):
       self.target_q_network.load_state_dict(self.q_network.state_dict())

In the constructor, two MLP networks are created, one for the Q-network and one for the target Q-network. Then, the weights of the target Q-network are updated to match the Q-network. The target Q-network is also set to evaluation mode, meaning it won't be updated during backpropagation. An optimizer (RMSprop) is also defined for updating the weights of the Q-network.
Training code:
ReplayBuffer Class:
Based on my research, it seems using continues samples to train doesn't give very good results, so what people normally do is to have a replaybuffer which stores past experience, and select samples from the buffer to train
The ReplayBuffer class is initialized with a buffer_size parameter, which determines the maximum length of the deque buffer used for storing experiences.
The sample method returns a random sample of experiences from the buffer, with the size of the sample determined by batch_size. It separates the experiences into separate lists for states, actions, rewards, next_states, and dones, which it then returns.
class ReplayBuffer:
   def __init__(self, buffer_size):
       self.buffer_size = buffer_size
       self.buffer = deque(maxlen=buffer_size)
   def add(self, state, action, reward, next_state, done):
       self.buffer.append((state, action, reward, next_state, done))
   def sample(self, batch_size):
       batch = random.sample(self.buffer, batch_size)
       states, actions, rewards, next_states, dones = [], [], [], [], []
       for experience in batch:
           state, action, reward, next_state, done = experience
           states.append(state)
           actions.append(action)
           rewards.append(reward)
           next_states.append(next_state)
           dones.append(done)
       return states, actions, rewards, next_states, dones
   def __len__(self):
       return len(self.buffer)


Main Training Loop:
task = 'hover'  # 'hover' or 'landing'


   max_m_episode = 800000
   max_steps = 800
   buffer_size = 10000
   batch_size = 64
   start_learning = 1000
   update_frequency = 4
   target_network_update_frequency = 100

These lines set various hyperparameters and settings for the reinforcement learning algorithm and environment.
Then creates an instance of the Rocket environment with the specified task and maximum steps per episode.
env = Rocket(task=task, max_steps=max_steps)

I initialized the DQN network with the dimensionality of the environment's state and action space. 
net = DQN(input_dim=env.state_dims, output_dim=env.action_dims).to(device)

The for loop runs for each episode until the maximum number of episodes. At the beginning of each episode, the environment is reset to an initial state, and the episode_reward is set to zero.
for episode_id in range(last_episode_id, max_m_episode):
 state = env.reset()
       episode_reward = 0


In the inner loop, the agent chooses an action based on the current state, executes the action in the environment to get the next state, reward, and a flag done indicating whether the episode has ended. This experience (state, action, reward, next_state, done) is then added to the replay buffer.

for step_id in range(max_steps):
           action = net.get_action(state)
           next_state, reward, done, _ = env.step(action)


           replay_buffer.add(state, action, reward, next_state, done)

Once the replay buffer has enough experiences (more than start_learning), the agent begins learning from the experiences. It samples a batch of experiences from the replay buffer every update_frequency steps
The actions from the sampled batch are converted to a tensor and moved to the device. The Q-values of the states from the batch are obtained by forwarding the states through the network, and the Q-values corresponding to the batch actions are selected.
batch = replay_buffer.sample(batch_size)
               batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = batch


               batch_actions = torch.tensor(batch_actions, dtype=torch.long).to(device)
               q_values = net(torch.tensor(batch_states, dtype=torch.float32).to(device)).gather(1, batch_actions.unsqueeze(1)).squeeze(1)

The Q-values of the next states are computed using the target Q-network. The maximum Q-value across all possible actions is selected for each next state in the batch.
next_q_values = net.target_q_network(torch.tensor(batch_next_states, dtype=torch.float32).to(device)).max(1)[0]

The rewards and done flags from the batch are converted to tensors and moved to the device. The target Q-values are calculated using the Bellman equation: the reward for taking the action plus the discounted maximum Q-value of the next state if the episode is not done. (the one I talked about earlier)
batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32).to(device)
 batch_dones = torch.tensor(batch_dones, dtype=torch.float32).to(device)
 target_q_values = batch_rewards + (1 - batch_dones) * net.gamma * next_q_values

The loss is calculated as the mean squared error (MSE) between the predicted Q-values and the target Q-values. The optimizer's gradients are zeroed out to prevent accumulation of gradients from previous steps, and then the loss is backpropagated through the network. The optimizer then updates the network's parameters based on the gradients. 
loss = F.mse_loss(q_values, target_q_values)
               net.optimizer.zero_grad()
               loss.backward()
               net.optimizer.step()


